{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will investigate UFO data over the last century to gain some insight.\n",
    "* Please use all the techniques we have learned in the class to preprocesss/clean the dataset <p style=\"color:blue\"><b>ufo_sightings_large.csv</b></p>. \n",
    "* After the dataset is preprocessed, please split the dataset into training sets and test sets\n",
    "* Fit KNN to the training sets. \n",
    "* Print the score of KNN on the test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset \"ufo_sightings_large.csv\" in pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Shape of the ufo_sightings\n",
      "==========================\n",
      "(4935, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ufo=pd.read_csv(\"ufo_sightings_large.csv\")\n",
    "# Print the shape of the Dataset\n",
    "print(\"==========================\")\n",
    "print(\"Shape of the ufo_sightings\")\n",
    "print(\"==========================\")\n",
    "print(ufo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking column types & Converting Column types\n",
    "Take a look at the UFO dataset's column types using the dtypes attribute. Please convert the column types to the proper types.\n",
    "For example, the date column, which can be transformed into the datetime type. \n",
    "That will make our feature engineering efforts easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Before changing the column types\n",
      "=================================\n",
      "date               object\n",
      "city               object\n",
      "state              object\n",
      "country            object\n",
      "type               object\n",
      "seconds           float64\n",
      "length_of_time     object\n",
      "desc               object\n",
      "recorded           object\n",
      "lat                object\n",
      "long              float64\n",
      "dtype: object\n",
      "=================================\n",
      "After changing the column types\n",
      "=================================\n",
      "date              datetime64[ns]\n",
      "city                      object\n",
      "state                     object\n",
      "country                   object\n",
      "type                      object\n",
      "seconds                  float64\n",
      "length_of_time            object\n",
      "desc                      object\n",
      "recorded                  object\n",
      "lat                       object\n",
      "long                     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TWe use the pandas package to call the method to_datetime() for converting the date column type to datetime\n",
    "# Similarly we can convert using the .astype()\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"Before changing the column types\")\n",
    "print(\"=================================\")\n",
    "# Check the column types\n",
    "print(ufo.dtypes)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "# print(ufo[[\"seconds\",\"date\"]].dtypes)\n",
    "print(\"=================================\")\n",
    "print(\"After changing the column types\")\n",
    "print(\"=================================\")\n",
    "# Check the column types\n",
    "print(ufo.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropping missing data\n",
    "Let's remove some of the rows where certain columns have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Shape of the Dataset after removing the missing data\n",
      "====================================================\n",
      "(3891, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dropping using the dropna()\n",
    "ufo_no_missing=ufo.dropna()\n",
    "print(\"====================================================\")\n",
    "print(\"Shape of the Dataset after removing the missing data\")\n",
    "print(\"====================================================\")\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing data\n",
    "\n",
    "# Check the if the column contains null value using isnull() and count the null values using the sum()\n",
    "# print(\"==================================================================================\")\n",
    "# print(\"Number of data missing in columns length_of_time, state, type, country, desc, city\")\n",
    "# print(\"==================================================================================\")\n",
    "#print(ufo[[\"length_of_time\", \"state\", \"type\",\"country\",\"desc\",\"city\"]].isnull().sum())\n",
    "\n",
    "# Keeping the rows which are not null\n",
    "# ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
    "        # ufo[\"state\"].notnull() &\n",
    "        # ufo[\"type\"].notnull()&ufo[\"country\"].notnull()&ufo[\"desc\"].notnull()&ufo[\"city\"].notnull()]\n",
    "\n",
    "# print(\"====================================================\")\n",
    "# print(\"Shape of the Dataset after removing the missing data\")\n",
    "# print(\"====================================================\")\n",
    "# Print out the shape of the new dataset\n",
    "# print(ufo_no_missing.shape)\n",
    "# Print the columns of the new dataset\n",
    "# print(\"=============================\")\n",
    "# print(\"Columns of the new data frame\")\n",
    "# print(\"=============================\")\n",
    "# print(ufo_no_missing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Seconds column after removing zeros\n",
      "===================================\n",
      "0    1209600.0\n",
      "1         30.0\n",
      "3        300.0\n",
      "5        600.0\n",
      "6        600.0\n",
      "Name: seconds, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# This is to remove the 0.0 values from the seconds columns which will impact the variance and calculating the score\n",
    "ufo_no_missing = ufo_no_missing[(ufo_no_missing[['seconds']] != 0).all(axis=1)]\n",
    "print(\"===================================\")\n",
    "print(\"Seconds column after removing zeros\")\n",
    "print(\"===================================\")\n",
    "print(ufo_no_missing[\"seconds\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting numbers from strings\n",
    "The <b>length_of_time</b> column in the UFO dataset is a text field that has the number of \n",
    "minutes within the string. \n",
    "Here, you'll extract that number from that text field using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Shape of new Dataset\n",
      "====================\n",
      "(3723, 12)\n",
      "=============================================================================\n",
      "New Dataset\n",
      "=============================================================================\n",
      "                 date       city state country      type    seconds  \\\n",
      "0 2011-11-03 19:21:00  woodville    wi      us   unknown  1209600.0   \n",
      "1 2004-10-03 19:05:00  cleveland    oh      us    circle       30.0   \n",
      "3 2002-11-21 05:45:00   clemmons    nc      us  triangle      300.0   \n",
      "5 2012-06-16 23:00:00  san diego    ca      us     light      600.0   \n",
      "6 2009-07-12 21:30:00     duluth    mn      us      oval      600.0   \n",
      "\n",
      "              length_of_time  \\\n",
      "0                    2 weeks   \n",
      "1                     30sec.   \n",
      "3            about 5 minutes   \n",
      "5                 10 minutes   \n",
      "6  total? maybe around 10 mi   \n",
      "\n",
      "                                                desc  recorded         lat  \\\n",
      "0  Red blinking objects similar to airplanes or s...  12/12/11  44.9530556   \n",
      "1               Many fighter jets flying towards UFO  10/27/04  41.4994444   \n",
      "3  It was a large&#44 triangular shaped flying ob...  12/23/02  36.0213889   \n",
      "5  Dancing lights that would fly around and then ...    7/4/12  32.7152778   \n",
      "6  A minor amber color trail&#44 (from where we w...   3/13/12  46.7833333   \n",
      "\n",
      "         long extracted_minutes  \n",
      "0  -92.291111               NaN  \n",
      "1  -81.695556               NaN  \n",
      "3  -80.382222   about 5 minutes  \n",
      "5 -117.156389        10 minutes  \n",
      "6  -92.106389               NaN  \n"
     ]
    }
   ],
   "source": [
    "# As the column \"length of time\" contains data in the form of weeks, hours, seconds, unrelevant data. \n",
    "# We will extract the data which is in the notation that has words \"minutes or minute\". We are not considering the \n",
    "# data which has min, mi, or h:mm pattern\n",
    "\n",
    "# Copying the data set into ufo_copy instead of modifying the original data\n",
    "\n",
    "ufo_copy=ufo_no_missing.copy()\n",
    "\n",
    "# Extracting the data with word minutes or minute\n",
    "ufo_copy[\"extracted_minutes\"]=ufo_copy[\"length_of_time\"].str.extract(\"(.*\\d+?.minutes|\\d.?minute)\",expand=False)\n",
    "\n",
    "# Printing the shape of the copied Dataset\n",
    "print(\"====================\")\n",
    "print(\"Shape of new Dataset\")\n",
    "print(\"====================\")\n",
    "print(ufo_copy.shape)\n",
    "\n",
    "# Looking into Dataset by using the head()\n",
    "print(\"=============================================================================\")\n",
    "print(\"New Dataset\")\n",
    "print(\"=============================================================================\")\n",
    "print(ufo_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Extracted_minutes and minutes columns from ufo_copy Dataset\n",
      "===========================================================\n",
      "  extracted_minutes  minutes\n",
      "0               NaN      NaN\n",
      "1               NaN      NaN\n",
      "3   about 5 minutes      NaN\n",
      "5        10 minutes     10.0\n",
      "6               NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "# The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. \n",
    "# Here, you'll extract that number from that text field using regular expressions.\n",
    "# Incase you want all the numbers from the \"length_of_time\" column comment the above section\n",
    "# Extracting the number from the \"extracted minutes\" column\n",
    "import re\n",
    "\n",
    "# By using the match() it'll ectract the data if it encoured in the first position instead of searching the whole string\n",
    "# By using the search() it'll extract the data at any position in the string\n",
    "\n",
    "# Defining the function return_minutes \n",
    "def return_minutes(time_string):\n",
    "            pattern = re.compile(r\"\\d+\")\n",
    "            num = re.match(pattern, str(time_string))\n",
    "            if num is not None:\n",
    "                return int(num.group(0))\n",
    "# Apply the extraction to the extracted_minutes column\n",
    "ufo_copy[\"minutes\"] = ufo_copy[\"extracted_minutes\"].apply(return_minutes)\n",
    "# Take a look at the head of both of the columns\n",
    "print(\"===========================================================\")\n",
    "print(\"Extracted_minutes and minutes columns from ufo_copy Dataset\")\n",
    "print(\"===========================================================\")\n",
    "print(ufo_copy[[\"extracted_minutes\",\"minutes\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identifying features for standardization \n",
    "In this section, you'll investigate the variance of columns in the UFO dataset to \n",
    "determine which features should be standardized. You can log normlize the high variance column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Columns of the copied dataset\n",
      "=============================\n",
      "Index(['date', 'city', 'state', 'country', 'type', 'seconds', 'length_of_time',\n",
      "       'desc', 'recorded', 'lat', 'long', 'extracted_minutes', 'minutes'],\n",
      "      dtype='object')\n",
      "===============================\n",
      "Variance of seconds and minutes\n",
      "===============================\n",
      "seconds    1.767437e+10\n",
      "dtype: float64\n",
      "minutes    112.782792\n",
      "dtype: float64\n",
      "=======================================\n",
      "Variance of seconds after normalization\n",
      "=======================================\n",
      "4.83635013345897\n"
     ]
    }
   ],
   "source": [
    "# Import the numpy package to use the log() for normalization\n",
    "import numpy as np\n",
    "\n",
    "# Printing the shape of the Dataset\n",
    "print(\"=============================\")\n",
    "print(\"Columns of the copied dataset\")\n",
    "print(\"=============================\")\n",
    "print(ufo_copy.columns)\n",
    "# Printing the variance of the columns of minutes and seconds\n",
    "print(\"===============================\")\n",
    "print(\"Variance of seconds and minutes\")\n",
    "print(\"===============================\")\n",
    "# print(ufo_copy[[\"seconds\",\"minutes\"]].var())\n",
    "print(ufo_copy[[\"seconds\"]].var())\n",
    "print(ufo_copy[[\"minutes\"]].var())\n",
    "# Performing the normalization on the seconds column using the log()\n",
    "ufo_copy[\"seconds_after_log\"] = np.log(ufo_copy[\"seconds\"])\n",
    "# print(ufo_copy[\"seconds_after_log\"])\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(\"=======================================\")\n",
    "print(\"Variance of seconds after normalization\")\n",
    "print(\"=======================================\")\n",
    "print(ufo_copy[\"seconds_after_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoding categorical variables\n",
    "There are couple of columns in the UFO dataset that need to be encoded before they can be \n",
    "modeled through scikit-learn. \n",
    "You'll do that transformation here, <b>using both binary and one-hot encoding methods</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Binary encoded of the country column\n",
      "====================================\n",
      "0    1\n",
      "1    1\n",
      "3    1\n",
      "5    1\n",
      "6    1\n",
      "Name: country_code_enc, dtype: int64\n",
      "==================================\n",
      "One-hot encoded of the type column\n",
      "==================================\n",
      "                 date       city state country      type    seconds  \\\n",
      "0 2011-11-03 19:21:00  woodville    wi      us   unknown  1209600.0   \n",
      "1 2004-10-03 19:05:00  cleveland    oh      us    circle       30.0   \n",
      "3 2002-11-21 05:45:00   clemmons    nc      us  triangle      300.0   \n",
      "5 2012-06-16 23:00:00  san diego    ca      us     light      600.0   \n",
      "6 2009-07-12 21:30:00     duluth    mn      us      oval      600.0   \n",
      "\n",
      "              length_of_time  \\\n",
      "0                    2 weeks   \n",
      "1                     30sec.   \n",
      "3            about 5 minutes   \n",
      "5                 10 minutes   \n",
      "6  total? maybe around 10 mi   \n",
      "\n",
      "                                                desc  recorded         lat  \\\n",
      "0  Red blinking objects similar to airplanes or s...  12/12/11  44.9530556   \n",
      "1               Many fighter jets flying towards UFO  10/27/04  41.4994444   \n",
      "3  It was a large&#44 triangular shaped flying ob...  12/23/02  36.0213889   \n",
      "5  Dancing lights that would fly around and then ...    7/4/12  32.7152778   \n",
      "6  A minor amber color trail&#44 (from where we w...   3/13/12  46.7833333   \n",
      "\n",
      "    ...     flash formation  light  other  oval  rectangle  sphere  teardrop  \\\n",
      "0   ...         0         0      0      0     0          0       0         0   \n",
      "1   ...         0         0      0      0     0          0       0         0   \n",
      "3   ...         0         0      0      0     0          0       0         0   \n",
      "5   ...         0         0      1      0     0          0       0         0   \n",
      "6   ...         0         0      0      0     1          0       0         0   \n",
      "\n",
      "   triangle  unknown  \n",
      "0         0        1  \n",
      "1         0        0  \n",
      "3         1        0  \n",
      "5         0        0  \n",
      "6         0        0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"====================================\")\n",
    "print(\"Binary encoded of the country column\")\n",
    "print(\"====================================\")\n",
    "# The LabelEncoder is used to Label the column names uniquely. As after dropping the rows which are null we are left \n",
    "# with three country names which will return [0,1,2]\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# country_enc = LabelEncoder()\n",
    "# ufo_copy[\"country_enc\"] = country_enc.fit_transform(ufo_copy[\"country\"])\n",
    "# print(ufo_copy[\"country_enc\"].unique())\n",
    "ufo_copy[\"country_code_enc\"] = np.where(ufo_copy[\"country\"].str.contains(\"us\"), 1, 0)\n",
    "print(ufo_copy[\"country_code_enc\"].head())\n",
    "#print(ufo_copy[\"country_code_enc\"].head())\n",
    "# Using the get_dummies() method from pandas package\n",
    "# ufo_copy[\"type\"] = pd.get_dummies(ufo_copy[\"type\"])\n",
    "ufo_copy_tenc=pd.get_dummies(ufo_copy[\"type\"])\n",
    "ufo_copy = pd.concat([ufo_copy, ufo_copy_tenc], axis=1)\n",
    "print(\"==================================\")\n",
    "print(\"One-hot encoded of the type column\")\n",
    "print(\"==================================\")\n",
    "print(ufo_copy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text vectorization (10 points)\n",
    "Let's transform the <b>desc</b> column in the UFO dataset into tf/idf vectors, \n",
    "since there's likely something we can learn from this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Count the null values in the desc column\n",
      "========================================\n",
      "desc    0\n",
      "dtype: int64\n",
      "=========================\n",
      "Look into the desc column\n",
      "=========================\n",
      "0    Red blinking objects similar to airplanes or s...\n",
      "1                 Many fighter jets flying towards UFO\n",
      "3    It was a large&#44 triangular shaped flying ob...\n",
      "5    Dancing lights that would fly around and then ...\n",
      "6    A minor amber color trail&#44 (from where we w...\n",
      "Name: desc, dtype: object\n",
      "===================================\n",
      "Shape of the transform_text_tfidf_v\n",
      "===================================\n",
      "(3723, 5209)\n"
     ]
    }
   ],
   "source": [
    "# Check how many values are missing in the desc columns\n",
    "print(\"========================================\")\n",
    "print(\"Count the null values in the desc column\")\n",
    "print(\"========================================\")\n",
    "print(ufo_copy[[\"desc\"]].isnull().sum())\n",
    "\n",
    "# Look at the desc column in the ufo Dataset\n",
    "print(\"=========================\")\n",
    "print(\"Look into the desc column\")\n",
    "print(\"=========================\")\n",
    "print(ufo_copy[\"desc\"].head())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create object for the TfidfVectorizer. tfidf_v is the short notion for the tfidf_vector\n",
    "tfidf_v = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf_idf vectors\n",
    "transform_text_tfidf_v = tfidf_v.fit_transform(ufo_copy[\"desc\"])\n",
    "\n",
    "print(\"===================================\")\n",
    "print(\"Shape of the transform_text_tfidf_v\")\n",
    "print(\"===================================\")\n",
    "# Print the transform_text_tfidf_v shape\n",
    "print(transform_text_tfidf_v.shape)\n",
    "# print(tfidf_v.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Selecting the ideal dataset \n",
    "Let's get rid of some of the unnecessary features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Date column of the ufo_copy Dataset\n",
      "===================================\n",
      "0   2011-11-03 19:21:00\n",
      "1   2004-10-03 19:05:00\n",
      "3   2002-11-21 05:45:00\n",
      "5   2012-06-16 23:00:00\n",
      "6   2009-07-12 21:30:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "============================================================\n",
      "After extraction of the Day, Month and Year ufo_copy Dataset\n",
      "============================================================\n",
      "                 date  day  month  year\n",
      "0 2011-11-03 19:21:00    3     11  2011\n",
      "1 2004-10-03 19:05:00    3     10  2004\n",
      "3 2002-11-21 05:45:00   21     11  2002\n",
      "5 2012-06-16 23:00:00   16      6  2012\n",
      "6 2009-07-12 21:30:00   12      7  2009\n"
     ]
    }
   ],
   "source": [
    "# Printing the date column in the ufo_copy Dataset\n",
    "print(\"===================================\")\n",
    "print(\"Date column of the ufo_copy Dataset\")\n",
    "print(\"===================================\")\n",
    "print(ufo_copy[\"date\"].head())\n",
    "# Extract day from the date column\n",
    "ufo_copy[\"day\"] = ufo_copy[\"date\"].apply(lambda row: row.day)\n",
    "# Extract month from the date column\n",
    "ufo_copy[\"month\"] = ufo_copy[\"date\"].apply(lambda row: row.month)\n",
    "# Extract year from the date column\n",
    "ufo_copy[\"year\"] = ufo_copy[\"date\"].apply(lambda row: row.year)\n",
    "# Print the \"date\", \"day\", \"month\",\"year\"\n",
    "print(\"============================================================\")\n",
    "print(\"After extraction of the Day, Month and Year ufo_copy Dataset\")\n",
    "print(\"============================================================\")\n",
    "print(ufo_copy[[\"date\", \"day\", \"month\",\"year\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Columns list in the ufo_copy Dataset\n",
      "====================================\n",
      "Index(['date', 'city', 'state', 'country', 'type', 'seconds', 'length_of_time',\n",
      "       'desc', 'recorded', 'lat', 'long', 'extracted_minutes', 'minutes',\n",
      "       'seconds_after_log', 'country_code_enc', 'changing', 'chevron', 'cigar',\n",
      "       'circle', 'cone', 'cross', 'cylinder', 'diamond', 'disk', 'egg',\n",
      "       'fireball', 'flash', 'formation', 'light', 'other', 'oval', 'rectangle',\n",
      "       'sphere', 'teardrop', 'triangle', 'unknown', 'day', 'month', 'year'],\n",
      "      dtype='object')\n",
      "=============================================================================\n",
      "Finding the correlation between seconds, extracted_minutes, seconds_after_log\n",
      "=============================================================================\n",
      "                    seconds  seconds_after_log\n",
      "seconds            1.000000           0.177998\n",
      "seconds_after_log  0.177998           1.000000\n",
      "===========================================\n",
      "Columns list in the ufo_copy_subset Dataset\n",
      "===========================================\n",
      "Index(['type', 'seconds_after_log', 'country_code_enc', 'changing', 'chevron',\n",
      "       'cigar', 'circle', 'cone', 'cross', 'cylinder', 'diamond', 'disk',\n",
      "       'egg', 'fireball', 'flash', 'formation', 'light', 'other', 'oval',\n",
      "       'rectangle', 'sphere', 'teardrop', 'triangle', 'unknown', 'day',\n",
      "       'month', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Printing the columns of the ufo_copy DataSet\n",
    "print(\"====================================\")\n",
    "print(\"Columns list in the ufo_copy Dataset\")\n",
    "print(\"====================================\")\n",
    "print(ufo_copy.columns)\n",
    "\n",
    "# Find the correlation between the columns\n",
    "print(\"=============================================================================\")\n",
    "print(\"Finding the correlation between seconds, extracted_minutes, seconds_after_log\")\n",
    "print(\"=============================================================================\")\n",
    "print(ufo_copy[[\"seconds\",\"extracted_minutes\",\"seconds_after_log\"]].corr())\n",
    "\n",
    "# Remove the redundant columns\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\",\"extracted_minutes\", \"lat\",\"length_of_time\",\"long\",\"minutes\", \"recorded\",\"seconds\",\"state\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "ufo_copy_subset = ufo_copy.drop(to_drop, axis=1)\n",
    "\n",
    "# # Printing the columns of the ufo_copy_subset DataSet\n",
    "print(\"===========================================\")\n",
    "print(\"Columns list in the ufo_copy_subset Dataset\")\n",
    "print(\"===========================================\")\n",
    "print(ufo_copy_subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Split the X and y using train_test_split, setting stratify = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ufo_copy_subset.drop([\"type\"],axis = 1)\n",
    "y = ufo_copy_subset[\"type\"].astype(str)\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fit knn to the training sets and print the score of knn on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Score of the test sets\n",
      "======================\n",
      "0.5146848137535817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)\n",
    "# Print the score of knn on the test sets\n",
    "print(\"======================\")\n",
    "print(\"Score of the test sets\")\n",
    "print(\"======================\")\n",
    "print(knn.score(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Score of the test sets\n",
      "======================\n",
      "0.14500537056928034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Split the dataset according to the class distribution of desc\n",
    "y = ufo_copy[\"type\"].astype(str)\n",
    "train_X, test_X, train_y, test_y = train_test_split(transform_text_tfidf_v.toarray(), y, stratify = y)\n",
    "nb = GaussianNB()\n",
    "# Fit the model to the training data\n",
    "nb.fit(train_X, train_y)\n",
    "print(\"======================\")\n",
    "print(\"Score of the test sets\")\n",
    "print(\"======================\")\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
